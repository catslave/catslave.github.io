---
layout: post
title: Netty
category: Netty
description: Netty线程模型、核心组件、相关参数。
---

## 什么是Netty

Netty是一个异步、事情驱动的网络编程框架，基于JDK NIO实现。对NIO进行了分装和优化，简化的网络编程的复杂度，提高了稳定和更高的性能。

**异步** 

Netty的异步是通过**回调**和**Future**实现的。
> 回调：Netty内部使用回调来处理事件，当事件发生时，对应事件Handler的方法实现会被调用。比如ConnectHandler的channelActive方法，当一个新连接被建立时，该方法将会被调用。

> Future：首先说明下Future的概念，future表示一个操作完成时通知应用程序的方式。

JDK原生Future
> jdk提供的future实现其实是需要应用程序手动去检查或者一直阻塞直到它完成。

Netty提供的Future-ChannelFuture
> Netty提供了自己的Future实现ChannelFuture，是基于事件监听来实现的。

> 比如客户端连接服务端事件，channel.connect方法返回的是一个channelFuture，表示连接是异步的。

> 我们通过给这个channelFuture添加事件监听器来监听连接是否成功。当连接成功时该监听器将会被调用。

**事件驱动**

使用事件来通知状态和操作的改变。
> Netty中所有操作的发生都是采用事件的形式来通知的，当一个事件发生时，对应ChannelHandler的方法将会被调用。

## IO模型

Netty 基于JDK NIO实现，JDK 1.4 NIO提供了非阻塞IO，基于多路复用的I/O实现。

I/O多路复用通过将多个I/O的阻塞复用到一个select的阻塞上，从而使用系统可以在单线程的情况下同时处理多个客户端请求。I/O多路复用最大的优势在于，系统开销小。

I/O多路复用是通过系统调用实现的，目前支持的系统调用有：select、poll和epoll。

### select模式

将需要监听的套接字和事件放到一个集合fd_set上，select方法会一直监听这些套接字，遍历整个集合检查是否有事件发生，如果有事件发生就立即返回对应事件。

select模式的问题：

1. 每次调用select函数，都需要把fd_set集合从用户态拷贝到内核态；
2. select函数需要在内核态中遍历整个fd_set集合，线性扫描；
3. 为了减少数据拷贝带来的性能损坏，内核对被监控的fd_set集合大小做了限制，默认为1024（可以通过修改系统参数进行限制）。

### poll模式

对select的优化，集合改用polldf链表结构存储，所以没有了最大文件描述符的限制，但是也要线性扫描整个链表。

### epoll模式

目前三者性能最优的一种模式，集合使用红黑树存储，没有最大文件描述符的限制。基于事件驱动实现，不需要扫描整个集合。当有事件发生时通过事件回调，将事件加入就绪队列，直接返回整个队列，队列使用双向链表结构存储。队列的返回即消息事件的通知，为了避免集合的来回拷贝，epoll使用了mmap技术，在用户空间和内核空间做了内存映射。

epoll的改进：
1. 没有最大文件描述符的限制（仅受限于操作系统的最大文件句柄数）；
2. I/O效率不会随着文件描述符的增加而线性下降；
3. 使用mmap加速内核与用户空间的消息传递。

## 线程模型

Netty线程模型，基于Reactor模型，同时支持Reactor单线程模型、多线程模型和主从多线程模型。Netty服务启动创建两个Reactor线程池，一个用于接收客户端的TCP连接，一个用于处理I/O请求。

## 核心组件

### EventLoop

Netty的EventLoop其实就是对JDK Selector的封装，Channel（socket）添加到EventLoop上，注册读/写事件，调用select方法开始轮询监听。EventLoop是一个I/O线程，除了负责I/O的读写外，还负责系统任务和定时任务的处理。

EventLoop的主要工作：
1. 轮询注册到selector上的所有channel的IO事件；
2. 处理产生IO事件的channel；
3. 处理任务队列。

#### EventLoopGroup

Netty的线程池实现，一个线程池维护多个EventLoop线程。Netty服务端启动创建两个EventLoopGroup，bossGroup负责Acceptor事件、wrokerGroup负责I/O事件。

EventLoopGroup内部使用ThreadPerTaskExecutor线程池，用于创建指定数量的EventLoop。

### Channel

bossGroup接收新的连接，将连接封装成channel交给workerGroup并注册读事件。

#### 主要的几个I/O操作

1. Channel.read()，从channel读取数据放入缓冲区（一个I/O操作），然后触发ChannelHandler.channelRead事件（一个I/O事件）。 
2. ChannelFuture.write(Object msg)，将数据写入缓冲区。注意，这里的缓冲区是指应用程序缓冲区，数据还在用户空间。flush方法才会将数据写入内核空间。
3. Channel.flush() 将应用程序缓冲区的数据写入内核缓冲区，即tcp发送缓冲区。

#### ChannelPipeline

Netty是基于事件驱动的，Channel进行I/O操作时会产生对应的I/O事件，事件会在ChannelPipeline中传播，然后对应的ChannelHandler对事件进行拦截和处理。

Netty中的事件分为I/O操作（网络I/O操作，比如读/写）和I/O事件（操作触发的各种事件）。I/O操作由DefaultChannelPipeline处理，I/O事件由ChannelPipeline处理。

ChannelPipeline基于责任链设计模式实现，结构是一个双向链表结构，链表上的每个节点实际是ChannelHandler处理器。

根据事件的流向，可以将Netty事件分为inbound事件和outbound事件。inbound事件由I/O线程触发，比如TCP连接、读等事件。outbound事件由用户线程触发，比如写事件等。

每个Channel都绑定一条ChannelPipeline，ChannelPipeline是线程安全的。因为一个Channel只会交给一个EventLoop线程处理。但是ChannelPipeline上的ChannelHandler不是线程安全的，因为ChannelHandler可以绑定到多个Pipeline上，会存在多线程并发调用情况。

#### Unsafe操作

Channel中的很多I/O操作都是交由Unsafe接口完成的。

### ByteBuf

Netty中的ByteBuf是对JDK ByteBuffer的封装，提供堆内、堆外内存，内存池优化技术。前面说的数据缓冲区指的就是ByteBuf，Channel.write方法先将数据写入ByteBuf，然后Channel.flush方法再将数据从ByteBuf写入内核发送缓冲区。

ByteBuf提供堆内和堆外两种缓冲实现：
- 堆内（HeapByteBuf）字节缓冲区：缓冲对象在JVM堆内存分配，速度快。缺点是写操作需要先将数据从堆内拷贝到堆外。
- 堆外（DirectByteBuf）字节缓冲区：直接在JVM堆外分配，速度慢。优点是减少一次内存拷贝。

ByteBuf的实践推荐，网络I/O的操作比如读/写使用堆外缓冲区，业务线程的数据使用堆内缓冲区。

为了提高ByteBuf的利用率，降低对ByteBuf的频繁分配和回收，提供了基于对象池的ByteBuf，可以重用ByteBuf对象，提升内存利用率。

## 参数调优

### ChannelOptions几个重要参数说明

1. ChannelOption.SO_BACKLOG 设置 socket函数的listen(int socketfd, int backlog)的backlog值。
2. ChannelOptions.SO_REUSEADDR 设置 SO_REUSEADDR 的值，允许复用IP地址和端口。
3. ChannelOptions.SO_KEEPALVE 设置 SO_KEEPALVE 的值，用于tcp连接的空闲检测。
4. ChannelOptions.SO_SNDBUF/SO_RVCBUF 设置SO_SNDBUF/SO_RVCBUF，接收和发送缓冲区的大小。
5. ChannelOptions.SO_LINGER 设置 SO_LINGER 的值，调用close方法的处理逻辑。linux默认会继续把缓冲区的数据发完。
6. ChannelOptions.TCP_NODELAY 设置 TCP_NODELAY 的值，关闭Nagle算法，立即发送数据。


## 高性能体现

1. 每个Channel对应一个EventLoop线程，Channel上的所有操作都是串行话，避免了多线程的并发操作，实现了无锁串行执行。
2. 基于Reactor主/从线程模型。
3. 内存优化，ByteBuf的堆外内存，ByteBuf的对象池技术。